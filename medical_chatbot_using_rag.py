# -*- coding: utf-8 -*-
"""Medical_Chatbot using RAG.ipynb

Automatically generated by Colab.


#Build BioMistral medical chatbot using Open source llm
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install langchain sentence-transformers chromadb transformers
!pip install -q 'git+https://github.com/huggingface/transformers@v4.32.0#egg=transformers' --no-deps

import langchain
from langchain.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.document_loaders import TextLoader
from langchain.llms import HuggingFaceHub
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import chromadb
from sentence_transformers import SentenceTransformer
import os

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load BioMistral model (adjust the path if it's not on HuggingFace)
model_name = "/content/drive/MyDrive/LLM_RAG_Medical_Chatbot/BioMistral-7B.Q8_0.gguf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Check model load
print("Model and tokenizer loaded successfully.")

# Load the Sentence-Transformer model for embeddings
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Example model

import chromadb
from langchain.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings

# Initialize Chroma client
client = chromadb.Client()

# Create the Chroma collection
collection_name = "medical_docs"
collection = client.create_collection(collection_name)

# Create embeddings from your documents
documents = ["Example medical document 1", "Example medical document 2"]  # Replace with actual medical data
embeddings = embedding_model.encode(documents)

# Add documents and embeddings to Chroma
for doc, emb in zip(documents, embeddings):
    collection.add([doc], embeddings=[emb])

print("Documents added to Chroma successfully.")

from langchain.chains import RetrievalQA

# Initialize SentenceTransformer embeddings and Chroma vector store
embedding = SentenceTransformerEmbeddings(embedding_model)
vector_store = Chroma(collection_name=collection_name, embedding_function=embedding)

# Set up the Retriever and QA Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=HuggingFaceHub(repo_id="huggingface/bio-mistral-7b"),
    retriever=vector_store.as_retriever()
)

# Test query
query = "What is the treatment for Type 2 diabetes?"
response = qa_chain.run(query)
print(response)